{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Act 1: Data-centric Workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll build a multimodal search pipeline using video as an example - a project that typically means managing separate systems: S3 for storage, a vector database to store vector embeddings (i.e., Pinecone, LanceDB), Postgres for metadata, plus an orchestration framework (i.e., Airflow, Prefect) to move data between them. With Pixeltable, you'll use a single system that handles storage, orchestration, and retrieval.\n",
    "\n",
    "While we use video in our example here, Pixeltable works equally well with: \n",
    "\n",
    "- images\n",
    "- audio files \n",
    "- documents like PDFs\n",
    "\n",
    "The concepts you'll learn apply to all data types.\n",
    "\n",
    "## In this notebook\n",
    "1. **Storage** - Create a video table with schema for video, images, and text\n",
    "2. **Orchestration** - Add computed columns that automatically calculate and persist video metadata\n",
    "3. **Retrieval: Basic Queries** - Write SQL-like queries to select and filter data from tables\n",
    "4. **Iterate: Create Views with Iterators** - Extract frames using iterators to create derived views\n",
    "5. **Retrieval: Visual Search** - Build embedding indexes for image similarity search\n",
    "6. **Retrieval: Cross-Modal Search** - Query with text to find similar images using CLIP embeddings\n",
    "7. **Persistent Storage** - Demonstrate how data and computed columns persist across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Here's what you'll build:**\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│  INPUT            EMBED              SEARCH                   RESULTS       │\n│                                                                             │\n│  ┌────────┐      ┌────────┐        ┌──────────────┐         ┌────────────┐  │\n│  │ Video  │─────▶│ CLIP   │───────▶│  Similarity  │────────▶│  Matching  │  │\n│  │ Frames │      │Vectors │        │    Search    │         │   Frames   │  │\n│  └────────┘      └────────┘        └──────────────┘         └────────────┘  │\n│      │               │                     │                       │        │\n│      │               │                     │                       │        │\n│  Extract         Generate            Query with              Ranked by      │\n│  frames          embeddings          reference               similarity     │\n│  from video      using CLIP          image/text              score          │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Storage: Create a Video Table\n",
    "\n",
    "First, we'll create a Pixeltable to store our video data. This provides persistent storage - your data and computed results are stored in the database. Learn more about [tables and data operations](https://docs.pixeltable.com/tutorials/tables-and-data-operations) and the [type system](https://docs.pixeltable.com/platform/type-system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable as pxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Uncomment to start fresh by dropping the directory\n",
    "# pxt.drop_dir('primetime-workshop', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a virtual directory to hold all our tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxt.create_dir('primetime-workshop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pxt.create_table(\n",
    "    'primetime-workshop/primetime_vids',\n",
    "    schema={'video': pxt.Video,\n",
    "            'title': pxt.String,\n",
    "            'promo_img': pxt.Image,\n",
    "            'promo_text': pxt.String},\n",
    "    if_exists='replace_force'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table is empty to start, but the schema is ready. We used [`create_table()`](https://docs.pixeltable.com/sdk/latest/pixeltable#func-create-table) to create a table with four columns, each of a different type. Learn more about [media types](https://docs.pixeltable.com/platform/type-system) supported by Pixeltable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a single local video file and build up our video processing pipeline using this table. \n",
    "\n",
    "While we're using a local file here, Pixeltable can work with files from anywhere - local paths, URLs, or cloud storage. The key thing is that Pixeltable works with file references and only downloads files to disk on access. Read more about working with remote files: https://docs.pixeltable.com/howto/cookbooks/data/data-import-s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.insert([{\n",
    "    'video': 'source/queens-gambit.mp4',\n",
    "    'title': 'The Queens Gambit',\n",
    "    'promo_img': 'source/queens-gambit-img.jpg',\n",
    "    'promo_text': 'Set during the Cold War era, orphaned chess prodigy Beth Harmon struggles with addiction in a quest to become the greatest chess player in the world.'\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the row we just inserted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Orchestration: Add Computed Columns\n",
    "\n",
    "[Computed columns](https://docs.pixeltable.com/tutorials/computed-columns) are derived columns that can represent a wide range of application specific logic - data transformations, external API calls, local model inference, etc. These columns are stored persistently and automatically (re)computed when needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Computed Columns Diagram](workshop-extras/computed-columns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Computed columns are defined with Pixeltable's expression language, which can be extended with:\n",
    "\n",
    "- user-defined functions (UDFs), \n",
    "- aggregate functions (UDAs), and \n",
    "- iterators. \n",
    "\n",
    "Pixeltable contains a large number of built-in UDFs that range from data/media type specific logic (video, audio, etc.; example: `get_duration(tbl_name.video_col)` for videos as we'll use here) to external API calls for all of the popular inference providers.\n",
    "\n",
    "You can find these under `pxt.functions` for each type of media: https://docs.pixeltable.com/sdk/latest/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable.functions as pxtf\n",
    "\n",
    "t.add_computed_column(duration=pxtf.video.get_duration(t.video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed columns are a key concept in Pixeltable. Computed columns can build on other computed columns. You can create an arbitrarily complex DAG, and Pixeltable will automatically parallelize the execution based on the dependencies implicit in the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With every computed column, the data that is computed is persisted, and so is the metadata. We can view the updated table schema to confirm that Pixeltable has stored this new metadata for our new computed column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.select(t.video, t.duration).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `duration` column is now a computed column. This means:\n",
    "\n",
    "- **Everything is stored persistently** - The value and the metadata are in the database, not just in memory\n",
    "- **It auto-updates** - For any new rows inserted, Pixeltable will compute the duration automatically\n",
    "- **It can be reused** - You can reference duration in other computed columns, and Pixeltable orchestrates the computation order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 - Retrieval: Write Queries\n",
    "\n",
    "You can also explore your data without storing results. [Queries](https://docs.pixeltable.com/tutorials/queries-and-expressions) let you experiment and iterate quickly. Here, we'll use the `extract_frame()` UDF for videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a frame at a specific timestamp (try different timestamps!)\n",
    "t.select(t.video, pxtf.video.extract_frame(t.video, timestamp=75)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting a frame in a query doesn't add a column to the table - it's just for exploration. The table schema remains unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - Iterate: Create Views with Iterators\n",
    "\n",
    "To search over frames, we need a table that contains frames. We'll create a view based on the video table that extracts frames from each video.\n",
    "\n",
    "![Views Diagram](workshop-extras/views.png)\n",
    "\n",
    "**What are Views?**\n",
    "- Materialized derived tables with cached, automatically updated results\n",
    "- Can filter base table with query predicates (dynamically maintained subset)\n",
    "- Can add computed columns to views - **useful when you want to do expensive operations only on a subset of your data**\n",
    "- *Note that you cannot manually insert rows into a view*\n",
    "\n",
    "**What are Iterator Views?**\n",
    "- Use iterators to decompose data: video → frames, document → text elements  \n",
    "- One base table row becomes multiple view rows (one per iterator element)\n",
    "- Every row gets a unique `pos` value for position\n",
    "- Example: insert 1 video → get 10 frame rows automatically\n",
    "- **In our use case:** We need a table of frames to search over, so we create a view that extracts 10 frames from each video\n",
    "\n",
    "**How Views Work:**\n",
    "- Views implicitly join with the base table without duplication - the base table columns are accessible through the view without duplicating any underlying data or values (Pixeltable executes parent-child join only when you query those columns)\n",
    "- Cannot directly insert or delete rows from views - new rows come from the base table\n",
    "- Automatically maintained when base table changes (insert a video → iterator runs → frame rows appear in view)\n",
    "\n",
    "We'll create a view based on the video table - it extracts 10 frames from each video using [`frame_iterator()`](https://docs.pixeltable.com/sdk/latest/video#iterator-frame-iterator).\n",
    "\n",
    "The view automatically updates if we insert new videos into the base table. This means if you insert a new video into the base table, the Pixeltable runtime system will then use the iterator to add the corresponding rows to the frame view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view with one row per frame\n",
    "fv = pxt.create_view(\n",
    "    'primetime-workshop/video-frame-view',\n",
    "    t,\n",
    "    iterator=pxtf.video.frame_iterator(t.video, num_frames=10),\n",
    "    if_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`frame_iterator()`](https://docs.pixeltable.com/sdk/latest/video#iterator-frame-iterator) outputs the frame plus relevant frame attributes, which are materialized as separate columns in the view. Let's check the view schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The view includes these new frame attributes as separate columns:\n",
    "\n",
    "- `frame_idx`: The frame index in the video\n",
    "- `pos_msec`: The position in milliseconds\n",
    "- `pos_frame`: The position in frame number\n",
    "- `frame`: The extracted frame image\n",
    "\n",
    "See the [`frame_iterator()` documentation](https://docs.pixeltable.com/sdk/latest/video#iterator-frame-iterator) for more details.\n",
    "\n",
    "You'll also notice that the columns from the base table are also present, but as mentioned earlier, they are not repeated. There is no data duplication here: Pixeltable does a parent-child join here against the base table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the extracted frames:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv.select(fv.frame_idx, fv.pos_msec, fv.pos_frame, fv.frame).limit(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 - Retrieval: Visual Search\n",
    "\n",
    "Now let's search videos visually. We want to find frames similar to any other frame or image.\n",
    "\n",
    "We'll build a searchable [embedding index](https://docs.pixeltable.com/platform/embedding-indexes) on these frames using CLIP.\n",
    "\n",
    "CLIP is a multimodal model from OpenAI that encodes images into embeddings that capture semantic meaning. This lets you find visually similar frames even if they don't look exactly alike - it understands what's in the image, not just pixel patterns.\n",
    "\n",
    "Embedding models in Pixeltable are just another kind of user-defined function (UDF). They accept an image and return an array. Pixeltable supports a variety of HuggingFace embedding models as UDFs, but you can bring your own embedding model too.\n",
    "\n",
    "More resources:\n",
    "- In addition to CLIP, Pixeltable includes several built-in UDFs for Hugging Face models: link\n",
    "- Learn more about [CLIP functions](https://docs.pixeltable.com/sdk/latest/huggingface#clip) and the [CLIP documentation](https://huggingface.co/docs/transformers/en/model_doc/clip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about creating your own UDFs, see the [UDF decorator documentation](https://docs.pixeltable.com/sdk/latest/pixeltable#decorator-udf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CLIP embedding index on frames for similarity search\n",
    "fv.add_embedding_index(\n",
    "    fv.frame,\n",
    "    embedding=pxtf.huggingface.clip.using(model_id='openai/clip-vit-base-patch32')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you declare the index, you don't need to update it manually. From now on, the index will reflect the rows in the view and the index is maintained incrementally automatically by Pixeltable. This means if you add or remove rows from the base table, the index will always be up to date with the view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform visual search. First, we'll extract a query frame from the video to search for similar frames. The `[0, 0]` syntax returns the first row of the first column from the result set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a query frame from the video at a timestamp\n",
    "# Try 75, 200, and 360\n",
    "query_frame = t.select(t.video.extract_frame(timestamp=75)).head()[0, 0]\n",
    "query_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's search for frames similar to our query frame.\n",
    "\n",
    "We use the `similarity()` pseudo-function in Pixeltable, which is available on any column with an embedding index. You apply it directly to a column, and the parameter is the search item. Here we are searching with an image. The range of parameter types here is determined by the embedding function when you created the index. CLIP supports both text and image searches, so you could use either here.\n",
    "\n",
    "If you have embedding functions that support a wider array of data types (like TwelveLabs, for example, which allows you to use audio or video for search), you can specify those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual search - find frames similar to the query frame\n",
    "sim = fv.frame.similarity(image=query_frame)\n",
    "\n",
    "similar = (\n",
    "    fv\n",
    "    .order_by(sim, asc=False)\n",
    "    .select(\n",
    "        fv.pos_msec,\n",
    "        fv.frame,\n",
    "        score=sim\n",
    "    )\n",
    "    .limit(4)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "similar  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 - Retrieval: Cross-Modal Search\n",
    "\n",
    "CLIP is a multimodal model - it can encode both images AND text into the same embedding space. This means you can search for frames using text queries, not just image queries.\n",
    "\n",
    "Let's search for frames using text descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text search - find frames using a text query\n",
    "# The similarity() function works with text strings too - CLIP encodes them into the same embedding space\n",
    "# Try different queries like: \"crowd\", \"chess game\", \"winter clothes\"\n",
    "text_query = \"crowd\"\n",
    "\n",
    "sim = fv.frame.similarity(string=text_query)\n",
    "\n",
    "similar = (\n",
    "    fv\n",
    "    .order_by(sim, asc=False)\n",
    "    .select(\n",
    "        fv.pos_msec,\n",
    "        fv.frame,\n",
    "        score=sim\n",
    "    )\n",
    "    .limit(4)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "similar  # See frames that match your text description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLIP UDF works with both image queries (like we did with `query_frame`) and text queries (like we just did with `\"crowd\"`). Images and text are encoded into the same semantic space, so you can search across modalities. Try different text queries to explore your video content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 - Persistent Storage Demo\n",
    "\n",
    "Your data and computed columns survive kernel restarts. This is different from typical notebook workflows where you lose everything when the kernel dies.\n",
    "\n",
    "Clear all your outputs, restart your kernel, then run the code below. Learn more about [persistence](https://docs.pixeltable.com/platform/persistence) and [database concepts](https://docs.pixeltable.com/platform/database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable as pxt\n",
    "pxt.list_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxt.list_tables('primetime-workshop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of these tables are retrievable because Pixeltable persistently stores all of our data and computed columns. Let's get the table we created using [`get_table()`](https://docs.pixeltable.com/sdk/latest/pixeltable#func-get-table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pxt.get_table('primetime-workshop/primetime_vids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the table - notice the computed `duration` column is still there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a query and collect to see the contents. All your computed columns are still there - no need to recompute expensive operations after a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "You built a complete visual search pipeline using Pixeltable:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  INPUT            EMBED              SEARCH                   RESULTS       │\n",
    "│                                                                             │\n",
    "│  ┌────────┐      ┌────────┐        ┌──────────────┐         ┌────────────┐  │\n",
    "│  │ Video  │─────▶│ CLIP   │───────▶│  Similarity  │────────▶│  Matching  │  │\n",
    "│  │ Frames │      │Vectors │        │    Search    │         │   Frames   │  │\n",
    "│  └────────┘      └────────┘        └──────────────┘         └────────────┘  │\n",
    "│      │               │                     │                       │        │\n",
    "│      │               │                     │                       │        │\n",
    "│  Extract         Generate            Query with              Ranked by      │\n",
    "│  frames          embeddings          reference               similarity     │\n",
    "│  from video      using CLIP          image/text              score          │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Each step is declarative:**\n",
    "- **Input**: Video data with all columns persists in the database\n",
    "- **Embed**: CLIP generates vector embeddings for each frame automatically\n",
    "- **Search**: Similarity queries work with both images (Visual Search) and text (Cross-Modal Search)\n",
    "- **Results**: Frames ranked by similarity score\n",
    "\n",
    "**Next up:** In Act 2, you'll add a second search modality—semantic search over audio transcripts.\n",
    "\n",
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "### Storage & Tables\n",
    "- [Tables and Data Operations](https://docs.pixeltable.com/tutorials/tables-and-data-operations) - Creating tables and inserting data\n",
    "- [Type System](https://docs.pixeltable.com/platform/type-system) - Media types including Video, Image, and Audio\n",
    "- [Configuration](https://docs.pixeltable.com/platform/configuration) - Configuring Pixeltable settings\n",
    "\n",
    "**How Media is Stored:**\n",
    "- PostgreSQL stores only file paths/URLs, never raw media data.\n",
    "- Inserted local files: path stored, original file remains in place.\n",
    "- Inserted URLs: URL stored, file downloaded to File Cache on first access.\n",
    "- Generated media (computed columns): saved to Media Store (default: local, configurable to S3/GCS/Azure per-column).\n",
    "- File Cache size: configure via `file_cache_size_g` in `~/.pixeltable/config.toml`. See [configuration guide](https://docs.pixeltable.com/platform/configuration).\n",
    "\n",
    "### Computed Columns\n",
    "- [Computed Columns Tutorial](https://docs.pixeltable.com/tutorials/computed-columns) - Understanding computed columns\n",
    "- [Video Functions](https://docs.pixeltable.com/sdk/latest/video) - Video operations like `get_duration()` and `extract_frame()`\n",
    "\n",
    "### Queries & Views\n",
    "- [Queries and Expressions](https://docs.pixeltable.com/tutorials/queries-and-expressions) - Querying your data\n",
    "- [Views Platform Guide](https://docs.pixeltable.com/platform/views) - Creating and using views\n",
    "- [frame_iterator()](https://docs.pixeltable.com/sdk/latest/video#iterator-frame-iterator) - Extracting frames from video\n",
    "\n",
    "### Search & Embeddings\n",
    "- [Embedding Indexes](https://docs.pixeltable.com/platform/embedding-indexes) - Building searchable indexes\n",
    "- [Similarity Search Cookbooks](https://docs.pixeltable.com/howto/cookbooks/search/search-similar-images) - Performing similarity search\n",
    "- [CLIP Functions](https://docs.pixeltable.com/sdk/latest/huggingface#udf-clip) - Using CLIP for image embeddings\n",
    "- [HuggingFace Integration](https://docs.pixeltable.com/sdk/latest/huggingface) - Working with HuggingFace models\n",
    "\n",
    "## Functions Used\n",
    "\n",
    "This notebook uses the following Pixeltable functions:\n",
    "\n",
    "- [`add_computed_column()`](https://docs.pixeltable.com/sdk/latest/pixeltable#add-computed-column) - Add computed columns to tables\n",
    "- [`add_embedding_index()`](https://docs.pixeltable.com/sdk/latest/pixeltable#add-embedding-index) - Create embedding indexes for similarity search\n",
    "- [`create_table()`](https://docs.pixeltable.com/sdk/latest/pixeltable#func-create-table) - Create new tables\n",
    "- [`create_view()`](https://docs.pixeltable.com/sdk/latest/pixeltable#func-create-view) - Create views from tables\n",
    "- [`extract_frame()`](https://docs.pixeltable.com/sdk/latest/video#udf-extract_frame) - Extract frames from video at specific timestamps\n",
    "- [`frame_iterator()`](https://docs.pixeltable.com/sdk/latest/video#iterator-frame-iterator) - Iterator to extract multiple frames from video\n",
    "- [`get_duration()`](https://docs.pixeltable.com/sdk/latest/video#udf-get_duration) - Get video duration\n",
    "- [`get_table()`](https://docs.pixeltable.com/sdk/latest/pixeltable#func-get-table) - Retrieve existing tables\n",
    "- [`list_tables()`](https://docs.pixeltable.com/sdk/latest/pixeltable#func-list-tables) - List all tables in the database\n",
    "- [`similarity()`](https://docs.pixeltable.com/sdk/latest/pixeltable#similarity) - Perform similarity search using embedding indexes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}